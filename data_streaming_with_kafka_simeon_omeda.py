# -*- coding: utf-8 -*-
"""Data Streaming with Kafka-simeon omeda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BrZ40BGiXtR9u16V2V60j1ShcJjtjkOW

## Data Streaming with Kafka-simeon omeda

The project aims to build a Kafka pipeline that can receive real-time data from
telecommunications mobile money transactions and process it for analysis. The pipeline should
be designed to handle high volumes of data and ensure that the data is processed efficiently.
"""

!pip install confluent-kafka

from confluent_kafka import Producer, Consumer

"""1. Set up a Kafka cluster: """

# Confluent Cloud configurations

bootstrap_servers = 'pkc-6ojv2.us-west4.gcp.confluent.cloud:9092'
security_protocol = 'SASL_SSL'
sasl_mechanism = 'PLAIN'
sasl_plain_username = 'LDPOBWYLLYGF25FR'
sasl_plain_password = 'epWu1JKhMYRuqI/Ia12wn18e2MPfg+rFYyP4lAk6DxTe7aEwdOjrAa7Po1a7+yO6'
topic = 'topic_1'

"""2. Develop a Kafka producer"""

# Producer configuration
conf_producer = Producer( {
    'bootstrap.servers': bootstrap_servers,
    'security.protocol': security_protocol,
    'sasl.mechanism': sasl_mechanism,
    'sasl.username': sasl_plain_username,
    'sasl.password': sasl_plain_password
})

# Consumer configuration
conf_consumer = Consumer( {
    'bootstrap.servers': bootstrap_servers,
    'security.protocol': security_protocol,
    'sasl.mechanism': sasl_mechanism,
    'sasl.username': sasl_plain_username,
    'sasl.password': sasl_plain_password,
    'group.id': 'learning'
})

# from kafka import KafkaProducer
from confluent_kafka import Producer, Consumer
import json

# Create Kafka producer
# producer = KafkaProducer(bootstrap_servers=['your_kafka_broker_address'])

conf_producer= {
    'bootstrap.servers': bootstrap_servers,
    'security.protocol': security_protocol,
    'sasl.mechanism': sasl_mechanism,
    'sasl.username': sasl_plain_username,
    'sasl.password': sasl_plain_password
}


# TEST THE SOLUTION

def delivery_report(err, msg):
    """Callback function to confirm delivery of message."""
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))

# Define sample CDR data
cdr_data = {
    "transaction_id": "12345",
    "sender_phone_number": "256777123456",
    "receiver_phone_number": "256772987654",
    "transaction_amount": 100000,
    "transaction_time": "2023-04-19 12:00:00"
}

# Define function to calculate averages
def calculate_averages(region_data):
    region_total = 0
    num_transactions = 0
    for transaction in region_data:
        region_total += transaction['transaction_amount']
        num_transactions += 1
    region_average = region_total / num_transactions
    return region_average

# Start streaming data and calculate averages by sender_phone_number
region_data = {}
while True:
    msg = conf_consumer.poll(1.0)
    if msg is None:
        continue
    if msg.error():
        print("Consumer error: {}".format(msg.error()))
        continue
    data = json.loads(msg.value().decode('utf-8'))
    if data['region'] not in region_data:
        region_data[data['region']] = []
    region_data[data['region']].append(data)
    for region in region_data:
        region_average = calculate_averages(region_data[region])
        conf_producer.produce('average_amount_by_region', json.dumps({'region': region, 'average_amount': region_average}))
    conf_producer.flush()


# Serialize CDR data to JSON
serialized_cdr = json.dumps(cdr_data).encode('utf-8')

# Produce CDRs to Kafka topic

def send_cdrs_to_kafka():
    """Sends CDRs to Kafka topic."""
    p = Producer(conf_producer)

    cdrs = cdr_data

    for cdr in cdrs:
        p.produce('topic_1', json.dumps({'region': region, 'average_amount': region_average})), callback=delivery_report)

    p.flush()

if __name__ == '__main__':
    send_cdrs_to_kafka()

"""Calculating transaction amount averages by sender_phone_number"""